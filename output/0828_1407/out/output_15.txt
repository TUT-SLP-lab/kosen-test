LLMがハルシネーションを起こしやすいプロンプトはどのようなプロンプトですか？

「LLMが幻覚を起こしやすいプロンプトには、欠陥のある学習データから派生したもの、文脈に対して不十分または不正確なもの、曖昧または矛盾したプロンプト、そして推測的または仮定的なシナリオが含まれます。」

仮説1: LLMが誤情報を生成するのは、トレーニングデータが不正確または偏っているためである。LLMの学習は過去のデータに基づくため、そのデータが信頼できない場合、誤った知識を反映してしまう。これは、誤ったトレーニングデータが原因であるプロンプトがLLMにホロシネートを引き起こす可能性が高い。

仮説2: コンテキストが不十分または不正確なプロンプトも、LLMのホロシネーションを招く。ユーザーが提供するプロンプトが曖昧であったり、前提条件が欠けている場合、LLMはその限られた情報を基にして誤った推論を行う。この場合も誤った情報を生成する可能性が高い。

仮説3: 矛盾したり曖昧なプロンプトは、LLMに混乱を引き起こし、誤った応答を導く。ユーザーが提示するプロンプトが明確でない場合、LLMはいくつかの解釈を試みるが、矛盾した指示によって不正確な情報を提供する場合が多い。このような状況ではホロシネーションが発生しやすい。

最終回答: プロンプトがホロシネーションを引き起こすのは、トレーニングデータの不正確さ、コンテキストの不足、または矛盾した指示が含まれているためである。これらの要因が重なることで、LLMは誤った情報を生成する可能性が高い。しかし、すべてのLLMが同じ反応を示すわけではなく、具体的なプロンプトの性質やモデルの特性によって結果は異なるため、一概に断定することは難しい。
