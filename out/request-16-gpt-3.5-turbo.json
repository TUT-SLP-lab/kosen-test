{
  "model": "gpt-3.5-turbo",
  "output": "LLM (Large Language Model) がハルシネーション（alucination）を起こしやすいプロンプトには、以下のような特徴がある可能性があります：\n\n1. 矛盾した情報や背景知識の欠如：プロンプトが情報不足であったり、矛盾した内容を含んでいる場合、LLMが本来の文脈から脱線して予測を行いやすくなります。\n\n2. 極端な要求や仮定：プロンプトが極端なシナリオや事態を要求する場合、LLMが現実とは異なる予測をしやすくなります。\n\n3. 一般的すぎるプロンプト：特定の文脈や条件が明確でない一般的なプロンプトは、LLMが自らの知識や想像力を駆使して予測する際に誤った情報を生成しやすい場合があります。\n\n以上のようなプロンプトを与えた場合、LLMがハルシネーションを起こしやすくなる可能性があることを留意することが重要です。",
  "input": "LLMがハルシネーションを起こしやすいプロンプトはどのようなプロンプトですか？"
}