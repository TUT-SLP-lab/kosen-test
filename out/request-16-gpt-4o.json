{
  "model": "gpt-4o",
  "output": "大規模言語モデル（LLM）がハルシネーションを起こしやすいプロンプトにはいくつかの特徴が考えられます。以下にその例を示します：\n\n1. **不完全または曖昧なプロンプト**:\n   - 質問が具体的でない場合や、多義的な言葉を使っている場合。\n   例: 「その出来事について教えてください」\n   （どの出来事か特定できないため、モデルが適当なストーリーを作り出すことがあります）\n\n2. **非常に専門的な或いは事実確認が難しい内容**:\n   - 特定の分野に関する高度な専門知識を必要とする場合や、事実確認が難しい場合。\n   例: 「1951年に行われた地下核実験の規模と影響について教えてください」\n   （データがほとんど存在しないか、誤解される可能性が高い）\n\n3. **誤った前提に基づく質問**:\n   - 前提が事実でないか、誤った情報を元にしている質問。\n   例: 「ネッシーはどのようにしてイギリスに持ち込まれたのですか？」\n   （ネッシーの実在は確認されておらず、モデルが架空のストーリーを作る可能性が高い）\n\n4. **動的情報を含む質問**:\n   - 時間と共に変化する最新情報や数値を求める質問。\n   例: 「今のニューヨークの株価はどうなっていますか？」\n   （モデルは静的なデータを学習しているため、最新の情報を提供することはできません）\n\n5. **都市伝説や陰謀論に関する質問**:\n   - 確かな情報源がなく、噂やフィクションに基づく内容を含む質問。\n   例: 「エリア51での宇宙人に関する実験内容を教えてください」\n   （確定的な情報がないため、普遍的な回答が難しく、架空の内容を作り出すことがある）\n\n6. **一般的でない固有名詞や専門用語を含む質問**:\n   - あまり知られていない名前や特殊な用語を含む質問。\n   例: 「クラグルフンスの最新の研究成果は？」\n   （架空の名前や用語に対する適当な回答を生成する可能性がある）\n\nこれらの特徴を持つプロンプトでは、LLMが正確ではない情報や架空の話を生成するリスクがあります。プロンプトを明確かつ具体的に設定し、可能な限り事実に基づいた質問をすることで、ハルシネーションのリスクを軽減することができます。",
  "input": "LLMがハルシネーションを起こしやすいプロンプトはどのようなプロンプトですか？"
}